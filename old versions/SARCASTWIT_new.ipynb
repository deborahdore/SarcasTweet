{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARCASTWIT_new.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DokjWUCvcIkQ",
        "5wh23uCQcxfP",
        "29_DCW9URUjb",
        "daSEOScGUKJo",
        "pDQJOzR4-Ncl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Define dependencies and constrains**"
      ],
      "metadata": {
        "id": "IfcrKr5Na8A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to download tweet from Twitter, first one must create an account and apply for **developer priviledges**. The application will grant the developer basic access the the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) which are not enough because it only allows the download of tweet of the last 7 days. Therefore, I've applied to the [Premium plan](https://developer.twitter.com/en/support/twitter-api/premium) which allows the download of 25k of tweets per month along with the use _full archive_ and the _30 days_ search API but with limited amout of request per month."
      ],
      "metadata": {
        "id": "TdELnf08cHP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ImtN6rXUazit"
      },
      "outputs": [],
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "COLLAB_DIR = \"/content/\"\n",
        "\n",
        "# File with Twitter project credentials\n",
        "CREDENTIALS = COLLAB_DIR+'credentials.yaml'\n",
        "CREDENTIALS_KEY = 'search_tweets_30_day_dev'\n",
        "\n",
        "# csv file where tweet downloaded will be saved\n",
        "DATASET = COLLAB_DIR+'dataset.csv'\n",
        "DATASET_ANNOTATED = COLLAB_DIR+'dataset_annotated.csv'\n",
        "SENTIPOLIC = COLLAB_DIR+'sentipolic.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0sYKlPLLjvx",
        "outputId": "8a6bee67-ee5a-40a9-8788-b45ef84b916b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "bdKPmsLJTYA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPUDzdbEL6zM",
        "outputId": "7c1ecc47-b0b3-4412-dc49-d544a62119e0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libenchant1c2a\n",
        "!pip install pyenchant\n",
        "!apt-get install hunspell-it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdt8lR_QBTm",
        "outputId": "d284c09c-c094-4dfd-8bc1-1fd1b0e0ecb0"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant1c2a is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7hhURdQB9v",
        "outputId": "f0c66d1b-d94e-4140-9844-139a4daa5b84"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GbgJr6MgmM",
        "outputId": "b5b3d267-8987-4801-c97e-24eefe736ddb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.15\" 2022-04-19\n",
            "OpenJDK Runtime Environment (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp==3.4.4 pyspark==3.0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5_Ww5areNnp",
        "outputId": "71fbdb58-6162-45c9-9fdd-d5de6445ef43"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spark-nlp==3.4.4 in /usr/local/lib/python3.7/dist-packages (3.4.4)\n",
            "Requirement already satisfied: pyspark==3.0.3 in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark==3.0.3) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparktorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Kuj86ieRBf",
        "outputId": "4f527b73-44e6-4776-f865-3ea3909ead61"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sparktorch in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from sparktorch) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sparktorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from sparktorch) (0.3.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from sparktorch) (1.1.4)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->sparktorch) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (2022.5.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->sparktorch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ekphrasis -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU0saXBGeS3W",
        "outputId": "cfeee592-8d68-4186-b2e4-3b4d80b1074f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.3.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark configurations"
      ],
      "metadata": {
        "id": "VdfBlXxLTgxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "OJzJeXfRMCPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import sparknlp\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark NLP\")\\\n",
        "    .master(\"local[4]\")\\\n",
        "    .config(\"spark.driver.memory\",\"16G\")\\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)"
      ],
      "metadata": {
        "id": "mkCNpht6bvLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "type(sc)"
      ],
      "metadata": {
        "id": "HYCtzalg-rGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Files from GitHub"
      ],
      "metadata": {
        "id": "DokjWUCvcIkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/dataset.csv\n",
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/other/sentipolic.csv\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/credentials/credentials.yaml\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/dataset/dataset_annotated.csv"
      ],
      "metadata": {
        "id": "EENgh8yncNzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# italian dictionary for lemmatization\n",
        "!wget https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-it.txt"
      ],
      "metadata": {
        "id": "5v7zGUIlnl4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Retrieve Tweet**\n",
        "\n",
        "\n",
        "> Following, some code cell will be annotated with *%% script false* in order to avoid their execution. Those cell concern the download of the tweets from Twitter. Even if this may not sound dangerous, I've finished the request at my disposal. Therefore, calling the Twitter API will produce an error. Also, please don't run them otherwise the output of the cell will be lost.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wh23uCQcxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import DataFrame"
      ],
      "metadata": {
        "id": "dNbMBnpfepl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import requests\n",
        "import json\n",
        "import yaml\n",
        "import csv\n",
        "import pdb\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "W2fMl9gSezXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First we must retrieve and validate the credentials that we will need to access the Twitter API. I've store the bearer token in a yaml file: *credentials.yaml*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gt6JZtFie0Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_credentials(credentials, key):\n",
        "  with open(credentials, \"r\") as stream:\n",
        "    try:\n",
        "        credentials = yaml.safe_load(stream)\n",
        "        return credentials[key]\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)"
      ],
      "metadata": {
        "id": "qOoIg7uZc2bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = handle_credentials(CREDENTIALS, CREDENTIALS_KEY)\n",
        "endpoint = credentials['endpoint'] # we will use this endpoint to search for the tweet\n",
        "print(endpoint)"
      ],
      "metadata": {
        "id": "i_UEONFUfOt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Second we must create the header for the request"
      ],
      "metadata": {
        "id": "J-RByZFOf9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_headers(credentials:dict):\n",
        "  headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {credentials[\"bearer_token\"]}'\n",
        "  }\n",
        "  return headers"
      ],
      "metadata": {
        "id": "5OYEqkb2fW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = handle_headers(credentials)\n",
        "headers"
      ],
      "metadata": {
        "id": "vfRm7V29gCJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Another parameter of the request is the query. The query determines which tweet will be returned in the response. In our case, we have 2 types of queries: the one that searches for sarcastic tweets and the one that returns non-sarcastic tweets"
      ],
      "metadata": {
        "id": "9yp2tFf9gOsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the query about sarcastic tweet I've chosen some keyword that, in my opion, are used to express sarcasm and/or irony (sarcasm is a sub-type of irony):\n",
        "\n",
        "\n",
        "1. sarcasmo (with or without #)\n",
        "2. ironia (with or without #)\n",
        "3. \"*ridiamo per non piangere*\"\n",
        "4. #coincidenze (.. io non credo) is mostly used to express sarcasm\n",
        "5. \"*qualquadra non cosa*\"\n",
        "\n",
        "Many studies also suggest that sarcasm can be found in tweet related to politics. Therefore, these seems very good starting point:\n",
        "1. monti, draghi, berlusconi (known italian prime minister)\n",
        "2. governo\n",
        "3. premier\n",
        "\n",
        "\n",
        "For non-sarcastic tweet, I've excluded all the possibile word that may refer to sarcasm.\n",
        "\n",
        "The list of operator used can be found in the [Twitter API documentation](https://developer.twitter.com/en/docs/twitter-api/enterprise/rules-and-filtering/operators-by-product)."
      ],
      "metadata": {
        "id": "jL_QG_G5Ew4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_query = '(#sarcasmo OR sarcasmo OR #ironia OR ironia OR \"ridiamo per non piangere\" \\\n",
        "                  OR #coincidenze OR \"qualquadra non cosa\" OR draghi OR monti OR berlusconi \\\n",
        "                  OR governo OR premier) lang:it -has:media'\n",
        "\n",
        "non_sarcasm_query = '-\"ridiamo per non piangere\" -sarcasmo -ironia -\"qualquadra non cosa\" lang:it -has:media'"
      ],
      "metadata": {
        "id": "B4Qan_sigKsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we can define the function that will handle the request and the dataframe where tweet will be stored.\n",
        "\n",
        "\n",
        "> Other parameters that we need in order to process the request are:\n",
        "- *max_result_per_page* : the maximum number of tweets per call \n",
        "- *next_token* : a token that if passed to the request will return the next page of results\n",
        "- I've defined a parameter *max_num_of_request* that will stop the call once that we've reached the desidered amount of calls. This must be done because the request at our disposal are not illimited. So we must be careful to the number of the request that we do\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1AItsWZphAXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_request(endpoint, headers, query, max_result_per_page, next_token = None):\n",
        "  \n",
        "  if next_token is not None:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "      \"next\": next_token\n",
        "    })\n",
        "  else:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "    })\n",
        "  \n",
        "  response = requests.post(endpoint, headers=headers, data=payload)\n",
        "\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "OVmgNDDch8Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tweet(response, label):\n",
        "  tweets = []\n",
        "  json_response = json.loads(response)\n",
        "  \n",
        "  if 'results' in response:\n",
        "    results = json_response[\"results\"]\n",
        "\n",
        "    for tweet in results:\n",
        "      # is tweet a retweet?\n",
        "      if 'retweeted_status' in tweet:\n",
        "        if tweet['retweeted_status']['truncated']:\n",
        "          text = tweet['retweeted_status']['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['retweeted_status']['text']\n",
        "      else:\n",
        "        if tweet['truncated']:\n",
        "          text = tweet['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['text']\n",
        "        \n",
        "      text = text.replace('\"', \"'\")\n",
        "      data = Tweet(tweet[\"id\"], f\"{text}\", label)\n",
        "      \n",
        "      tweets.append(data)\n",
        "\n",
        "  else:\n",
        "    print(\"Request went wrong\")\n",
        "    print(response)\n",
        "\n",
        "  return tweets"
      ],
      "metadata": {
        "id": "1gNdo7Bnijtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   query, \n",
        "                   label,\n",
        "                   max_result_per_page,\n",
        "                   tweet_list,\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 20):\n",
        "\n",
        "  if max_num_of_request <= 0:\n",
        "    return tweet_list\n",
        "\n",
        "  response = handle_request(endpoint, headers, query, max_result_per_page, next_token)\n",
        "\n",
        "  tweet_list.extend(extract_tweet(response, label))\n",
        "\n",
        "  try:\n",
        "      next_token = json.loads(response)['next']\n",
        "  except:\n",
        "      next_token = None\n",
        "\n",
        "  if next_token is not None:\n",
        "      return download_tweet(endpoint, headers, query, label, max_result_per_page,\n",
        "                   tweet_list, next_token, max_num_of_request - 1)\n",
        "  else:\n",
        "      return tweet_list"
      ],
      "metadata": {
        "id": "oHbiTTbNg7x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tweet\n",
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "yUW4eUSoyq08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []"
      ],
      "metadata": {
        "id": "nnd9wSFdmzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download sarcastic tweet\n",
        "tweets = download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   sarcasm_query, \n",
        "                   \"Yes\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40)"
      ],
      "metadata": {
        "id": "wqphEQDcwHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download non-sarcastic tweet\n",
        "tweets.extend(\n",
        "    download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   non_sarcasm_query, \n",
        "                   \"No\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40))"
      ],
      "metadata": {
        "id": "rLUvtri88LyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(tweets)"
      ],
      "metadata": {
        "id": "RZeSJmGt5FOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "nmS3eNL075po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# create file\n",
        "if not os.path.exists(DATASET):\n",
        "  os.mknod(DATASET)\n",
        "\n",
        "# save tweets\n",
        "df.toPandas().to_csv(DATASET, header=True, index=False) "
      ],
      "metadata": {
        "id": "PgAKUhbDmrmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Annotate Tweet**"
      ],
      "metadata": {
        "id": "29_DCW9URUjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# python widgets\n",
        "from ipywidgets import Button\n",
        "import asyncio\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import HBox, Layout\n",
        "import time as t"
      ],
      "metadata": {
        "id": "p5GSk4dNelIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we download tweet using an hashtag, we are not 100% sure of what we downloaded is correct. We must analyze - at least - the majority of the tweet to understand if what we have labelled is correct. There here's a little tool to help us with that."
      ],
      "metadata": {
        "id": "xrDr46suRbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")\n",
        "\n",
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df = spark.createDataFrame(pd.read_csv(DATASET), schema=schema)\n",
        "df.show(10)"
      ],
      "metadata": {
        "id": "8hS8XFLNRa2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_label(df, numeric=False):\n",
        "  label_yes = 1 if numeric else \"Yes\"\n",
        "  label_no = 0 if numeric else \"No\"\n",
        "  return df.groupBy(\"sarcastic\").agg(\n",
        "      count(when(col(\"sarcastic\") == label_yes, 1)),\n",
        "      count(when(col(\"sarcastic\") == label_no, 1)))"
      ],
      "metadata": {
        "id": "g-2CxhPEb59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count tweet\n",
        "print(f'Total number of tweet retrieved {df.count()}')"
      ],
      "metadata": {
        "id": "UWC7f6AHbzKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want first to drop duplicates\n",
        "\n",
        "print(\"Count before drop:\")\n",
        "count_label(df).show()\n",
        "\n",
        "count_before_drop = df.count()\n",
        "df = df.dropDuplicates([\"text\"])\n",
        "print(f\"Distinct count: {str(df.count())} \\n\")\n",
        "\n",
        "print(\"Count after drop:\")\n",
        "count_label(df).show()"
      ],
      "metadata": {
        "id": "gciiYKT7R148"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dropped {count_before_drop-df.count()} columns')\n",
        "print(f'total count: {df.count()}')"
      ],
      "metadata": {
        "id": "q3WedH78qpMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visually \n",
        "data = count_label(df).collect()\n",
        "\n",
        "labels = ['sarcastic', 'non sarcastic']\n",
        "colors = sns.color_palette('pastel')[0:5]\n",
        "\n",
        "plt.pie([int(data[1][1]), int(data[0][2])], labels = labels, colors = colors, autopct='%.0f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "idZSd7XIVbXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_annotated = []"
      ],
      "metadata": {
        "id": "t4hgqk7qs9mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_change(widget1, widget2): \n",
        "    future = asyncio.Future()\n",
        "    def getvalue(change):\n",
        "        future.set_result(change.description)\n",
        "        widget1.on_click(getvalue, remove=True)\n",
        "        widget2.on_click(getvalue, remove=True) \n",
        "    widget1.on_click(getvalue)\n",
        "    widget2.on_click(getvalue)\n",
        "    return future\n",
        "\n",
        "async def f(df):\n",
        "  df_pandas = df.toPandas()\n",
        "  for index, row in df_pandas.iterrows():\n",
        "    print(f'Is this tweet sarcastic? \\n {row.text} \\n', flush=True)\n",
        "\n",
        "    x = await wait_for_change(sarcastic,non_sarcastic)\n",
        "    \n",
        "    if x == \"Yes\":\n",
        "      print(\"Tagged \", row.id, \"with sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"Yes\")\n",
        "      tweets_annotated.append(data)\n",
        "    else:\n",
        "      print(\"Tagged \", row.id, \"with non-sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"No\")      \n",
        "      tweets_annotated.append(data)\n",
        "\n",
        "    clear_output()\n",
        "    display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "T3QQC7zoSvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going forward, we want to ask ourselves *How can know if a tweet is sarcastic or not?*\n",
        "\n",
        "*In Harry Potter and the Half Blood Prince, there is a scene where Harry is leaving the Weasley house and Mrs. Weasley says: “Promise me you will look after yourself…stay out of trouble….” Harry responds: “I always do Mrs. Weasley. I like a quiet life, you know me.” Anyone familiar with Harry Potter knows that his life is far from quiet, and so he must not really mean what he is saying. In fact, Harry is being sarcastic.*\n",
        "\n",
        "[source](https://kids.frontiersin.org/articles/10.3389/frym.2018.00056)\n",
        "\n",
        "Sarcasm is the use of words that say the opposite of what you really mean, often as a joke and with a tone of voice that shows this. It is often used to mock or critize someone, express disapproval or as a defence mechanism.\n",
        "\n",
        "For example:\n",
        "> *Noi invece ce la caviamo con un grado in meno ai termosifoni d'inverno e spegnendo i condizionatori d'estate. Non è fantastico? (#Draghi è un cialtrone sesquipedale, nel caso aveste ancora qualche dubbio)*\n",
        "\n",
        "Here we can imagine the sarcastic tone of the writer. He's obviously criticising the Italian prime minister, Mario Draghi, when, during an interview, he said that we must make sacrifices like lowering the grade of the radiator in order to cope with the possibility of not having the gas from Russia anymore. Obviously, this won't be enough. *Isn't this great?*\n",
        "\n",
        "Sometimes it's difficult also for a human person to understand sarcasm therefore I don't expect the following dataset to be 100% free from bias."
      ],
      "metadata": {
        "id": "8Kyh9cjvkoZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool used for annotation: it displays each tweet and the user has to click \"Yes\" \n",
        "# if the tweet was sarcastic, \"No\" otherwise\n",
        "\n",
        "sarcastic=Button(description=\"Yes\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "non_sarcastic=Button(description=\"No\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "\n",
        "asyncio.create_task(f(df))\n",
        "t.sleep(2)\n",
        "display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "NJM-MObHrwWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "print(tweets_annotated)"
      ],
      "metadata": {
        "id": "XvlYa09uXPRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df_annotated = spark.createDataFrame(tweets_annotated)\n",
        "df_annotated.tail(5)"
      ],
      "metadata": {
        "id": "5oOAAGZFsYKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "if not os.path.exists(DATASET_ANNOTATED):\n",
        "  os.mknod(DATASET_ANNOTATED)\n",
        "\n",
        "# save tweets\n",
        "df_annotated.toPandas().to_csv(DATASET_ANNOTATED, header=True, index=False) "
      ],
      "metadata": {
        "id": "ClMe-OHFXrDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Extend Dataset**"
      ],
      "metadata": {
        "id": "daSEOScGUKJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df_annotated = spark.createDataFrame(pd.read_csv(DATASET_ANNOTATED), schema=schema)"
      ],
      "metadata": {
        "id": "W4g8iOQTxneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Annotated tweets: {df_annotated.count()}\")"
      ],
      "metadata": {
        "id": "K421QQz1sfyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the code below, we lost multiple *tweet*.\n",
        "First of all, multiple tweets classified as sarcastic were not sarcastic. Also, I've dropped every tweet that contained only one word, that wasn't actually in italian or \n",
        "that had no sense."
      ],
      "metadata": {
        "id": "M_awlh0kq3nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_annotated).show()"
      ],
      "metadata": {
        "id": "k6k_llNRpKCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can integrate we some external Dataset such as: [SENTIPOLIC](http://www.di.unito.it/~tutreeb/sentipolc-evalita16/index.html) from the challenge EVALITA2016 which contains several italian tweet already classified."
      ],
      "metadata": {
        "id": "UeH_4XijrNc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic = spark.createDataFrame(pd.read_csv(SENTIPOLIC))"
      ],
      "metadata": {
        "id": "B2eVp6uIw9kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "qld5kvU0yDBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will extract only the tweets which are ironic since we have plenty non-ironic\n",
        "df_sentipolic = df_sentipolic.filter(col(\"iro\")==1)"
      ],
      "metadata": {
        "id": "0LMvvrxNyFlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ironic tweet retrieved: {df_sentipolic.count()}\")"
      ],
      "metadata": {
        "id": "lN-MoSRiyQ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns that we don't need\n",
        "df_sentipolic = df_sentipolic.drop(*('subj', 'opos', 'oneg', 'lpos', 'lneg', 'top'))\n",
        "\n",
        "# rename columns\n",
        "df_sentipolic = df_sentipolic.withColumnRenamed(\"idTwitter\", \"id\")\\\n",
        "                              .withColumnRenamed(\"iro\", \"sarcastic\")\n",
        "\n",
        "# change order\n",
        "df_sentipolic = df_sentipolic.select(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "CmzhGiatyZxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "PrDm7Oz_zbDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to join the two dataset. However we must use the same label for both.\n",
        "# Therefore if the tweet is sarcastic, the label will be 1, 0 otherwise.\n",
        "\n",
        "\n",
        "df_annotated = df_annotated.withColumn(\"sarcastic\", \n",
        "                                         when(df_annotated.sarcastic == \"Yes\", 1)\n",
        "                                         .when(df_annotated.sarcastic == \"No\", 0)                                    \n",
        "                                         .otherwise(df_annotated.sarcastic))"
      ],
      "metadata": {
        "id": "9J2Ekp4317OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotated.show()"
      ],
      "metadata": {
        "id": "MEk2bAHu2XwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate DataFrames\n",
        "\n",
        "df_complete = df_annotated.union(df_sentipolic)\n",
        "df_complete.show(5)"
      ],
      "metadata": {
        "id": "vRXyeqRH2nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Now we have a total of {df_complete.count()} tweets')"
      ],
      "metadata": {
        "id": "DGDmh6_d3Csy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_complete, numeric=True).show()"
      ],
      "metadata": {
        "id": "e89JUhcA3IkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is still unbalanced, but better than before."
      ],
      "metadata": {
        "id": "Ap6Pm9b13WEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Data Processing**"
      ],
      "metadata": {
        "id": "pDQJOzR4-Ncl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enchant\n",
        "from enchant.checker import SpellChecker"
      ],
      "metadata": {
        "id": "aoDyZxNUem8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, explode"
      ],
      "metadata": {
        "id": "YXhRoVgner4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ],
      "metadata": {
        "id": "IaWr_68cghQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we want to clean tweet: remove hashtag, links, emoji, whitespaces, mentions."
      ],
      "metadata": {
        "id": "lGGkaFhE360S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,\n",
        "    segmenter=\"twitter\", \n",
        "    corrector=\"twitter\", \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    spell_correct_elong=False,\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "id": "GvCH8z28iLtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@udf\n",
        "def spell_checker(text):\n",
        "  checker = SpellChecker(\"it_IT\", text)\n",
        "  for err in checker:\n",
        "    if len(err.suggest())>0:\n",
        "      sug = err.suggest()[0]\n",
        "      err.replace(sug)\n",
        "  return checker.get_text()"
      ],
      "metadata": {
        "id": "dDwr9bgG5e4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@udf\n",
        "def tweet_processor(s):\n",
        "  return \" \".join(text_processor.pre_process_doc(s))"
      ],
      "metadata": {
        "id": "KbI8vITbgMc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning(df):\n",
        "  print('1. Convert to lowercase')\n",
        "  df = df.withColumn('text', lower(col('text')))\n",
        "\n",
        "  print('2. Remove links')\n",
        "  df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
        "\n",
        "  print(\"3. Remove unwanted symbols\")\n",
        "  df = df.withColumn('text', regexp_replace('text', '\\n', ''))\n",
        "\n",
        "  print(\"4. Remove digits\")\n",
        "  df = df.withColumn('text', regexp_replace('text', r'[0-9]{5,}', ''))\n",
        "\n",
        "  #print(\"5. Remove punctuation\")\n",
        "  #df = df.withColumn('text', regexp_replace('text', '[^a-zA-Z\\\\s]', ''))\n",
        "\n",
        "  '''\n",
        "  When annotating the tweets, I've noticed that many of them contained spelling \n",
        "  errors. It is recommended to adjust those tweets before the model training.\n",
        "  '''\n",
        "  #print(\"6. Spell Checker\")\n",
        "  #df = df.withColumn('text', spell_checker(col('text')))\n",
        "\n",
        "  print(\"7. Trimming\")\n",
        "  df = df.withColumn('text', trim(col('text')))\n",
        "\n",
        "  print(\"8. Filter out extra whitespaces\")\n",
        "  df = df.withColumn('text', regexp_replace(col(\"text\"), \" +\", \" \"))\n",
        "\n",
        "  print(\"9. Text processor\")\n",
        "  '''\n",
        "  it allows to normalize the text replacing each term with a fixed tuple \n",
        "  < [entitytype] > and to enclosing hashtags with two tags \n",
        "  < hashtag > ... < /hashtag >.\n",
        "  '''\n",
        "  df = df.withColumn('text', tweet_processor(col('text')))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "cvUnNbAxfXdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broker = enchant.Broker()\n",
        "broker.describe()\n",
        "broker.list_languages()"
      ],
      "metadata": {
        "id": "mmwTu7-xsNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = text_cleaning(df_complete)"
      ],
      "metadata": {
        "id": "p-nRPl8VhAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.cache()"
      ],
      "metadata": {
        "id": "-sM-GZ6khIQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "3kQw6QQspzZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Feature Engineering**"
      ],
      "metadata": {
        "id": "p-cnH0NX7Xup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries for feature engineering\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import Tokenizer"
      ],
      "metadata": {
        "id": "h0e1vrW9euor"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting feature engineering, constructing pipeline..\")"
      ],
      "metadata": {
        "id": "OjDEMuzht4po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a21fa4-4ede-4989-c514-ba6a22547932"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature engineering, constructing pipeline..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_pipeline():\n",
        "  document_assembler = DocumentAssembler()\\\n",
        "                        .setInputCol('text')\\\n",
        "                        .setOutputCol('document')\\\n",
        "                        .setCleanupMode(\"shrink\")\n",
        "  sentence_detector = SentenceDetector()\\\n",
        "                      .setInputCols('document')\\\n",
        "                      .setOutputCol('sentence')\n",
        "\n",
        "  tokenizer = sparknlp.annotator.Tokenizer()\\\n",
        "                    .setInputCols([\"sentence\"])\\\n",
        "                    .setOutputCol(\"token\")\n",
        "  \n",
        "  lemmatizer = Lemmatizer()\\\n",
        "     .setInputCols(['token'])\\\n",
        "     .setOutputCol('lemma')\\\n",
        "     .setDictionary(\"lemmatization-it.txt\", \"->\", \"\\t\")\n",
        "\n",
        "  stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_it\", \"it\")\\\n",
        "     .setInputCols(['lemma'])\\\n",
        "     .setOutputCol('clean_lemma')\n",
        "\n",
        "  finisher = Finisher()\\\n",
        "            .setInputCols(\"clean_lemma\")\\\n",
        "            .setOutputCols(\"pipeline_result\")\\\n",
        "            .setCleanAnnotations(False)\n",
        "\n",
        "  return Pipeline(stages=[\n",
        "                    document_assembler,\n",
        "                    sentence_detector,\n",
        "                    tokenizer,\n",
        "                    lemmatizer,\n",
        "                    stopwords_cleaner,\n",
        "                    finisher])"
      ],
      "metadata": {
        "id": "QuKOyId_hS4Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = construct_pipeline()"
      ],
      "metadata": {
        "id": "i284tX2NRJGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8248b8-69c0-4af2-e0a5-28aaa32919de"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords_it download started this may take some time.\n",
            "Approximate size to download 2.4 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nlp_pipeline = pipeline.fit(df_cleaned)\n",
        "df_engineer = nlp_pipeline.transform(df_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSgK1Cg7lLJ3",
        "outputId": "45b4b8c8-608f-45ad-b56b-efc9d584feed"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 146 ms, sys: 20.9 ms, total: 167 ms\n",
            "Wall time: 1.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_engineer.cache()\n",
        "df_engineer.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR7aTfrTst3c",
        "outputId": "95baeb46-8b1e-4f34-e023-bd51782c4937"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------------------------------------------------------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "|id                 |text                                                                                                               |sarcastic|document                                                                                                                                                      |sentence                                                                                                                                                      |token                                                                                                                                                      |lemma                                                                                                                                                      |clean_lemma                                                                                                                                                |pipeline_result                                                                                                      |\n",
            "+-------------------+-------------------------------------------------------------------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "|1524033834416754689|tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica     |0        |[[document, 0, 109, tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica, [sentence -> 0], []]]     |[[document, 0, 109, tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica, [sentence -> 0], []]]     |[[token, 0, 109, tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica, [sentence -> 0], []]]     |[[token, 0, 109, tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica, [sentence -> 0], []]]     |[[token, 0, 109, tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica, [sentence -> 0], []]]     |[tuseiungiocatoreincredibileabbracciamiemozionantediscorsodiunbambinoalcompagnodibattitoattualizzantelapolitica]     |\n",
            "|1524034020417294337|maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi|0        |[[document, 0, 114, maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi, [sentence -> 0], []]]|[[document, 0, 114, maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi, [sentence -> 0], []]]|[[token, 0, 114, maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi, [sentence -> 0], []]]|[[token, 0, 114, maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi, [sentence -> 0], []]]|[[token, 0, 114, maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi, [sentence -> 0], []]]|[maggioinoccasionedellaixgiornatadell'amiciziatracoptiecattolicimessaggioinviatooggidapierfrancescoaquadrottaiileggi]|\n",
            "|1524034242963034112|maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti               |0        |[[document, 0, 99, maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti, [sentence -> 0], []]]                |[[document, 0, 99, maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti, [sentence -> 0], []]]                |[[token, 0, 99, maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti, [sentence -> 0], []]]                |[[token, 0, 99, maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti, [sentence -> 0], []]]                |[[token, 0, 99, maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti, [sentence -> 0], []]]                |[maggiomelensonatocrimeapacehadetonatoriesportazioneavaligiabluperilservizioresoallachiarezzadeifatti]               |\n",
            "|1524034526908981250|ascuoladigiuseppecontedevivenire                                                                                   |1        |[[document, 0, 31, ascuoladigiuseppecontedevivenire, [sentence -> 0], []]]                                                                                    |[[document, 0, 31, ascuoladigiuseppecontedevivenire, [sentence -> 0], []]]                                                                                    |[[token, 0, 31, ascuoladigiuseppecontedevivenire, [sentence -> 0], []]]                                                                                    |[[token, 0, 31, ascuoladigiuseppecontedevivenire, [sentence -> 0], []]]                                                                                    |[[token, 0, 31, ascuoladigiuseppecontedevivenire, [sentence -> 0], []]]                                                                                    |[ascuoladigiuseppecontedevivenire]                                                                                   |\n",
            "|1524034207923814401|aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria                            |0        |[[document, 0, 86, aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria, [sentence -> 0], []]]                             |[[document, 0, 86, aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria, [sentence -> 0], []]]                             |[[token, 0, 86, aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria, [sentence -> 0], []]]                             |[[token, 0, 86, aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria, [sentence -> 0], []]]                             |[[token, 0, 86, aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria, [sentence -> 0], []]]                             |[aginevraglosannatraraccordodisublacenseesansepolcristaproblemiditrafficoveicoloinavaria]                            |\n",
            "+-------------------+-------------------------------------------------------------------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCuSZ9oAkOTX",
        "outputId": "e9cd6bb8-5a50-4abc-bf7b-ea1bffa942c5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, text: string, sarcastic: string]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "XpXlUVIbmiQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Feed Forward Network"
      ],
      "metadata": {
        "id": "2B3lKKqfmkHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding"
      ],
      "metadata": {
        "id": "5igAJuBomoGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = WordEmbeddings.pretrained(\"glove_6B_300\", \"xx\") \\\n",
        "      .setInputCols(\"sentence\", \"token\") \\\n",
        "      .setOutputCol(\"embeddings\")"
      ],
      "metadata": {
        "id": "ILuXH_5-mmSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}