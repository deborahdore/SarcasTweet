{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARCASTWIT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Define dependencies and constrains**"
      ],
      "metadata": {
        "id": "IfcrKr5Na8A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to download tweet from Twitter, first one must create an account and apply for **developer priviledges**. The application will grant the developer basic access the the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) which are not enough because it only allows the download of tweet of the last 7 days. Therefore, I've applied to the [Premium plan](https://developer.twitter.com/en/support/twitter-api/premium) which allows the download of 25k of tweets per month along with the use _full archive_ and the _30 days_ search API but with limited amout of request per month."
      ],
      "metadata": {
        "id": "TdELnf08cHP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ImtN6rXUazit"
      },
      "outputs": [],
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "COLLAB_DIR = \"/content/\"\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# File with Twitter project credentials\n",
        "CREDENTIALS = '/content/credentials.yaml'\n",
        "CREDENTIALS_KEY = 'search_tweets_30_day_dev'\n",
        "\n",
        "# csv file where tweet downloaded will be saved\n",
        "DATASET = '/content/dataset.csv'\n",
        "DATASET_ANNOTATED = '/content/dataset_annotated.csv'\n",
        "SENTIPOLIC = '/content/sentipolic.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0sYKlPLLjvx",
        "outputId": "4e3e3dc3-d2b5-42de-eb43-0034c84e8e41"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install libraries"
      ],
      "metadata": {
        "id": "bdKPmsLJTYA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libenchant1c2a\n",
        "!pip install pyenchant\n",
        "!apt-get install hunspell-it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdt8lR_QBTm",
        "outputId": "625ccf2a-f012-415a-db08-3ff9e22d3df7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant1c2a is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "hunspell-it is already the newest version (1:6.0.3-3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7hhURdQB9v",
        "outputId": "e3eef83d-5496-46d7-e937-ac8382922596"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GbgJr6MgmM",
        "outputId": "0461bfc1-f43b-45c6-d3fd-2d7993630c78"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.15\" 2022-04-19\n",
            "OpenJDK Runtime Environment (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.0\n",
        "!pip install spark-nlp==3.4.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvmO7zGx6v_1",
        "outputId": "6ff0ecba-cdf1-4bc5-b90e-4da61506edce"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.2.0 in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark==3.2.0) (0.10.9.2)\n",
            "Requirement already satisfied: spark-nlp==3.4.4 in /usr/local/lib/python3.7/dist-packages (3.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNZLejN9TRN2",
        "outputId": "5f567342-70bd-445e-b2c8-4e93d5073d81"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tqdm in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (4.64.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import libraries"
      ],
      "metadata": {
        "id": "IEOD3tq6TdtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark packages\n",
        "from pyspark import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import DataFrame"
      ],
      "metadata": {
        "id": "V-dnnL8JS90g"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing useful packages\n",
        "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, transform\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker"
      ],
      "metadata": {
        "id": "Oa1b_SYrUrr9"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries for feature engineering\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from sparknlp.base import LightPipeline"
      ],
      "metadata": {
        "id": "qmEjIQRep142"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import requests\n",
        "import json\n",
        "import yaml\n",
        "import csv\n",
        "import pdb\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "2Q2kd-HsTAVu"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# python widgets\n",
        "from ipywidgets import Button\n",
        "import asyncio\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import HBox, Layout\n",
        "import time as t\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "metadata": {
        "id": "9oWwzEUWTJGE"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "Qd8-rb9ZW3d9"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "L_SpAtHtRiUi"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tqdm import TQDMNotebookCallback"
      ],
      "metadata": {
        "id": "8bOshskATS8l"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark configurations"
      ],
      "metadata": {
        "id": "VdfBlXxLTgxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start(spark32=True)\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)"
      ],
      "metadata": {
        "id": "mkCNpht6bvLp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "7e39e56f-4791-4c22-a2d5-3fb157434bd4"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-c769c6848fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark32\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spark NLP version\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Apache Spark version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(gpu, spark23, spark24, spark32, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkRealTimeOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mspark_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_without_realtime_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparknlp/__init__.py\u001b[0m in \u001b[0;36mstart_without_realtime_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jsl.settings.storage.cluster_tmp_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_tmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_with_realtime_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setConfString'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd ~/.ivy2/cache/com.johnsnowlabs.nlp/spark-nlp_2.12/jars && ls -lt"
      ],
      "metadata": {
        "id": "0yAIKiS36-3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "1BYBCQ8C-JMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Files from GitHub"
      ],
      "metadata": {
        "id": "DokjWUCvcIkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/dataset.csv\n",
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/other/sentipolic.csv\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/credentials/credentials.yaml\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/dataset/dataset_annotated.csv"
      ],
      "metadata": {
        "id": "EENgh8yncNzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# italian dictionary for lemmatization\n",
        "!wget https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-it.txt"
      ],
      "metadata": {
        "id": "5v7zGUIlnl4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Retrieve Tweet**\n",
        "\n",
        "\n",
        "> Following, some code cell will be annotated with *%% script false* in order to avoid their execution. Those cell concern the download of the tweets from Twitter. Even if this may not sound dangerous, I've finished the request at my disposal. Therefore, calling the Twitter API will produce an error. Also, please don't run them otherwise the output of the cell will be lost.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wh23uCQcxfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First we must retrieve and validate the credentials that we will need to access the Twitter API. I've store the bearer token in a yaml file: *credentials.yaml*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gt6JZtFie0Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_credentials(credentials, key):\n",
        "  with open(credentials, \"r\") as stream:\n",
        "    try:\n",
        "        credentials = yaml.safe_load(stream)\n",
        "        return credentials[key]\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)"
      ],
      "metadata": {
        "id": "qOoIg7uZc2bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = handle_credentials(CREDENTIALS, CREDENTIALS_KEY)\n",
        "endpoint = credentials['endpoint'] # we will use this endpoint to search for the tweet\n",
        "print(endpoint)"
      ],
      "metadata": {
        "id": "i_UEONFUfOt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Second we must create the header for the request"
      ],
      "metadata": {
        "id": "J-RByZFOf9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_headers(credentials:dict):\n",
        "  headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {credentials[\"bearer_token\"]}'\n",
        "  }\n",
        "  return headers"
      ],
      "metadata": {
        "id": "5OYEqkb2fW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = handle_headers(credentials)\n",
        "headers"
      ],
      "metadata": {
        "id": "vfRm7V29gCJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Another parameter of the request is the query. The query determines which tweet will be returned in the response. In our case, we have 2 types of queries: the one that searches for sarcastic tweets and the one that returns non-sarcastic tweets"
      ],
      "metadata": {
        "id": "9yp2tFf9gOsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the query about sarcastic tweet I've chosen some keyword that, in my opion, are used to express sarcasm and/or irony (sarcasm is a sub-type of irony):\n",
        "\n",
        "\n",
        "1. sarcasmo (with or without #)\n",
        "2. ironia (with or without #)\n",
        "3. \"*ridiamo per non piangere*\"\n",
        "4. #coincidenze (.. io non credo) is mostly used to express sarcasm\n",
        "5. \"*qualquadra non cosa*\"\n",
        "\n",
        "Many studies also suggest that sarcasm can be found in tweet related to politics. Therefore, these seems very good starting point:\n",
        "1. monti, draghi, berlusconi (known italian prime minister)\n",
        "2. governo\n",
        "3. premier\n",
        "\n",
        "\n",
        "For non-sarcastic tweet, I've excluded all the possibile word that may refer to sarcasm.\n",
        "\n",
        "The list of operator used can be found in the [Twitter API documentation](https://developer.twitter.com/en/docs/twitter-api/enterprise/rules-and-filtering/operators-by-product)."
      ],
      "metadata": {
        "id": "jL_QG_G5Ew4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_query = '(#sarcasmo OR sarcasmo OR #ironia OR ironia OR \"ridiamo per non piangere\" \\\n",
        "                  OR #coincidenze OR \"qualquadra non cosa\" OR draghi OR monti OR berlusconi \\\n",
        "                  OR governo OR premier) lang:it -has:media'\n",
        "\n",
        "non_sarcasm_query = '-\"ridiamo per non piangere\" -sarcasmo -ironia -\"qualquadra non cosa\" lang:it -has:media'"
      ],
      "metadata": {
        "id": "B4Qan_sigKsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we can define the function that will handle the request and the dataframe where tweet will be stored.\n",
        "\n",
        "\n",
        "> Other parameters that we need in order to process the request are:\n",
        "- *max_result_per_page* : the maximum number of tweets per call \n",
        "- *next_token* : a token that if passed to the request will return the next page of results\n",
        "- I've defined a parameter *max_num_of_request* that will stop the call once that we've reached the desidered amount of calls. This must be done because the request at our disposal are not illimited. So we must be careful to the number of the request that we do\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1AItsWZphAXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_request(endpoint, headers, query, max_result_per_page, next_token = None):\n",
        "  \n",
        "  if next_token is not None:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "      \"next\": next_token\n",
        "    })\n",
        "  else:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "    })\n",
        "  \n",
        "  response = requests.post(endpoint, headers=headers, data=payload)\n",
        "\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "OVmgNDDch8Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tweet(response, label):\n",
        "  tweets = []\n",
        "  json_response = json.loads(response)\n",
        "  \n",
        "  if 'results' in response:\n",
        "    results = json_response[\"results\"]\n",
        "\n",
        "    for tweet in results:\n",
        "      # is tweet a retweet?\n",
        "      if 'retweeted_status' in tweet:\n",
        "        if tweet['retweeted_status']['truncated']:\n",
        "          text = tweet['retweeted_status']['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['retweeted_status']['text']\n",
        "      else:\n",
        "        if tweet['truncated']:\n",
        "          text = tweet['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['text']\n",
        "        \n",
        "      text = text.replace('\"', \"'\")\n",
        "      data = Tweet(tweet[\"id\"], f\"{text}\", label)\n",
        "      \n",
        "      tweets.append(data)\n",
        "\n",
        "  else:\n",
        "    print(\"Request went wrong\")\n",
        "    print(response)\n",
        "\n",
        "  return tweets"
      ],
      "metadata": {
        "id": "1gNdo7Bnijtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   query, \n",
        "                   label,\n",
        "                   max_result_per_page,\n",
        "                   tweet_list,\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 20):\n",
        "\n",
        "  if max_num_of_request <= 0:\n",
        "    return tweet_list\n",
        "\n",
        "  response = handle_request(endpoint, headers, query, max_result_per_page, next_token)\n",
        "\n",
        "  tweet_list.extend(extract_tweet(response, label))\n",
        "\n",
        "  try:\n",
        "      next_token = json.loads(response)['next']\n",
        "  except:\n",
        "      next_token = None\n",
        "\n",
        "  if next_token is not None:\n",
        "      return download_tweet(endpoint, headers, query, label, max_result_per_page,\n",
        "                   tweet_list, next_token, max_num_of_request - 1)\n",
        "  else:\n",
        "      return tweet_list"
      ],
      "metadata": {
        "id": "oHbiTTbNg7x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tweet\n",
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "yUW4eUSoyq08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []"
      ],
      "metadata": {
        "id": "nnd9wSFdmzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download sarcastic tweet\n",
        "tweets = download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   sarcasm_query, \n",
        "                   \"Yes\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40)"
      ],
      "metadata": {
        "id": "wqphEQDcwHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download non-sarcastic tweet\n",
        "tweets.extend(\n",
        "    download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   non_sarcasm_query, \n",
        "                   \"No\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40))"
      ],
      "metadata": {
        "id": "rLUvtri88LyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(tweets)"
      ],
      "metadata": {
        "id": "RZeSJmGt5FOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "nmS3eNL075po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# create file\n",
        "if not os.path.exists(DATASET):\n",
        "  os.mknod(DATASET)\n",
        "\n",
        "# save tweets\n",
        "df.toPandas().to_csv(DATASET, header=True, index=False) "
      ],
      "metadata": {
        "id": "PgAKUhbDmrmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Annotate Tweet**"
      ],
      "metadata": {
        "id": "29_DCW9URUjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we download tweet using an hashtag, we are not 100% sure of what we downloaded is correct. We must analyze - at least - the majority of the tweet to understand if what we have labelled is correct. There here's a little tool to help us with that."
      ],
      "metadata": {
        "id": "xrDr46suRbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")\n",
        "\n",
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df = spark.createDataFrame(pd.read_csv(DATASET), schema=schema)\n",
        "df.show(10)"
      ],
      "metadata": {
        "id": "8hS8XFLNRa2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_label(df, numeric=False):\n",
        "  label_yes = 1 if numeric else \"Yes\"\n",
        "  label_no = 0 if numeric else \"No\"\n",
        "  return df.groupBy(\"sarcastic\").agg(\n",
        "      count(when(col(\"sarcastic\") == label_yes, 1)),\n",
        "      count(when(col(\"sarcastic\") == label_no, 1)))"
      ],
      "metadata": {
        "id": "g-2CxhPEb59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count tweet\n",
        "print(f'Total number of tweet retrieved {df.count()}')"
      ],
      "metadata": {
        "id": "UWC7f6AHbzKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want first to drop duplicates\n",
        "\n",
        "print(\"Count before drop:\")\n",
        "count_label(df).show()\n",
        "\n",
        "count_before_drop = df.count()\n",
        "df = df.dropDuplicates([\"text\"])\n",
        "print(f\"Distinct count: {str(df.count())} \\n\")\n",
        "\n",
        "print(\"Count after drop:\")\n",
        "count_label(df).show()"
      ],
      "metadata": {
        "id": "gciiYKT7R148"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dropped {count_before_drop-df.count()} columns')\n",
        "print(f'total count: {df.count()}')"
      ],
      "metadata": {
        "id": "q3WedH78qpMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visually \n",
        "data = count_label(df).collect()\n",
        "\n",
        "labels = ['sarcastic', 'non sarcastic']\n",
        "colors = sns.color_palette('pastel')[0:5]\n",
        "\n",
        "plt.pie([int(data[1][1]), int(data[0][2])], labels = labels, colors = colors, autopct='%.0f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "idZSd7XIVbXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_annotated = []"
      ],
      "metadata": {
        "id": "t4hgqk7qs9mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_change(widget1, widget2): \n",
        "    future = asyncio.Future()\n",
        "    def getvalue(change):\n",
        "        future.set_result(change.description)\n",
        "        widget1.on_click(getvalue, remove=True)\n",
        "        widget2.on_click(getvalue, remove=True) \n",
        "    widget1.on_click(getvalue)\n",
        "    widget2.on_click(getvalue)\n",
        "    return future\n",
        "\n",
        "async def f(df):\n",
        "  df_pandas = df.toPandas()\n",
        "  for index, row in df_pandas.iterrows():\n",
        "    print(f'Is this tweet sarcastic? \\n {row.text} \\n', flush=True)\n",
        "\n",
        "    x = await wait_for_change(sarcastic,non_sarcastic)\n",
        "    \n",
        "    if x == \"Yes\":\n",
        "      print(\"Tagged \", row.id, \"with sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"Yes\")\n",
        "      tweets_annotated.append(data)\n",
        "    else:\n",
        "      print(\"Tagged \", row.id, \"with non-sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"No\")      \n",
        "      tweets_annotated.append(data)\n",
        "\n",
        "    clear_output()\n",
        "    display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "T3QQC7zoSvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going forward, we want to ask ourselves *How can know if a tweet is sarcastic or not?*\n",
        "\n",
        "*In Harry Potter and the Half Blood Prince, there is a scene where Harry is leaving the Weasley house and Mrs. Weasley says: “Promise me you will look after yourself…stay out of trouble….” Harry responds: “I always do Mrs. Weasley. I like a quiet life, you know me.” Anyone familiar with Harry Potter knows that his life is far from quiet, and so he must not really mean what he is saying. In fact, Harry is being sarcastic.*\n",
        "\n",
        "[source](https://kids.frontiersin.org/articles/10.3389/frym.2018.00056)\n",
        "\n",
        "Sarcasm is the use of words that say the opposite of what you really mean, often as a joke and with a tone of voice that shows this. It is often used to mock or critize someone, express disapproval or as a defence mechanism.\n",
        "\n",
        "For example:\n",
        "> *Noi invece ce la caviamo con un grado in meno ai termosifoni d'inverno e spegnendo i condizionatori d'estate. Non è fantastico? (#Draghi è un cialtrone sesquipedale, nel caso aveste ancora qualche dubbio)*\n",
        "\n",
        "Here we can imagine the sarcastic tone of the writer. He's obviously criticising the Italian prime minister, Mario Draghi, when, during an interview, he said that we must make sacrifices like lowering the grade of the radiator in order to cope with the possibility of not having the gas from Russia anymore. Obviously, this won't be enough. *Isn't this great?*\n",
        "\n",
        "Sometimes it's difficult also for a human person to understand sarcasm therefore I don't expect the following dataset to be 100% free from bias."
      ],
      "metadata": {
        "id": "8Kyh9cjvkoZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool used for annotation: it displays each tweet and the user has to click \"Yes\" \n",
        "# if the tweet was sarcastic, \"No\" otherwise\n",
        "\n",
        "sarcastic=Button(description=\"Yes\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "non_sarcastic=Button(description=\"No\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "\n",
        "asyncio.create_task(f(df))\n",
        "t.sleep(2)\n",
        "display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "NJM-MObHrwWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "print(tweets_annotated)"
      ],
      "metadata": {
        "id": "XvlYa09uXPRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df_annotated = spark.createDataFrame(tweets_annotated)\n",
        "df_annotated.tail(5)"
      ],
      "metadata": {
        "id": "5oOAAGZFsYKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "if not os.path.exists(DATASET_ANNOTATED):\n",
        "  os.mknod(DATASET_ANNOTATED)\n",
        "\n",
        "# save tweets\n",
        "df_annotated.toPandas().to_csv(DATASET_ANNOTATED, header=True, index=False) "
      ],
      "metadata": {
        "id": "ClMe-OHFXrDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Extend Dataset**"
      ],
      "metadata": {
        "id": "daSEOScGUKJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df_annotated = spark.createDataFrame(pd.read_csv(DATASET_ANNOTATED), schema=schema)"
      ],
      "metadata": {
        "id": "W4g8iOQTxneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Annotated tweets: {df_annotated.count()}\")"
      ],
      "metadata": {
        "id": "K421QQz1sfyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the code below, we lost multiple *tweet*.\n",
        "First of all, multiple tweets classified as sarcastic were not sarcastic. Also, I've dropped every tweet that contained only one word, that wasn't actually in italian or \n",
        "that had no sense."
      ],
      "metadata": {
        "id": "M_awlh0kq3nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_annotated).show()"
      ],
      "metadata": {
        "id": "k6k_llNRpKCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can integrate we some external Dataset such as: [SENTIPOLIC](http://www.di.unito.it/~tutreeb/sentipolc-evalita16/index.html) from the challenge EVALITA2016 which contains several italian tweet already classified."
      ],
      "metadata": {
        "id": "UeH_4XijrNc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic = spark.createDataFrame(pd.read_csv(SENTIPOLIC))"
      ],
      "metadata": {
        "id": "B2eVp6uIw9kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "qld5kvU0yDBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will extract only the tweets which are ironic since we have plenty non-ironic\n",
        "df_sentipolic = df_sentipolic.filter(col(\"iro\")==1)"
      ],
      "metadata": {
        "id": "0LMvvrxNyFlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ironic tweet retrieved: {df_sentipolic.count()}\")"
      ],
      "metadata": {
        "id": "lN-MoSRiyQ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns that we don't need\n",
        "df_sentipolic = df_sentipolic.drop(*('subj', 'opos', 'oneg', 'lpos', 'lneg', 'top'))\n",
        "\n",
        "# rename columns\n",
        "df_sentipolic = df_sentipolic.withColumnRenamed(\"idTwitter\", \"id\")\\\n",
        "                              .withColumnRenamed(\"iro\", \"sarcastic\")\n",
        "\n",
        "# change order\n",
        "df_sentipolic = df_sentipolic.select(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "CmzhGiatyZxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "PrDm7Oz_zbDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to join the two dataset. However we must use the same label for both.\n",
        "# Therefore if the tweet is sarcastic, the label will be 1, 0 otherwise.\n",
        "\n",
        "\n",
        "df_annotated = df_annotated.withColumn(\"sarcastic\", \n",
        "                                         when(df_annotated.sarcastic == \"Yes\", 1)\n",
        "                                         .when(df_annotated.sarcastic == \"No\", 0)                                    \n",
        "                                         .otherwise(df_annotated.sarcastic))"
      ],
      "metadata": {
        "id": "9J2Ekp4317OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotated.show()"
      ],
      "metadata": {
        "id": "MEk2bAHu2XwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate DataFrames\n",
        "\n",
        "df_complete = df_annotated.union(df_sentipolic)\n",
        "df_complete.show(5)"
      ],
      "metadata": {
        "id": "vRXyeqRH2nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Now we have a total of {df_complete.count()} tweets')"
      ],
      "metadata": {
        "id": "DGDmh6_d3Csy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_complete, numeric=True).show()"
      ],
      "metadata": {
        "id": "e89JUhcA3IkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is still unbalanced, but better than before."
      ],
      "metadata": {
        "id": "Ap6Pm9b13WEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Data Processing**"
      ],
      "metadata": {
        "id": "pDQJOzR4-Ncl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we want to clean tweet: remove hashtag, links, emoji, whitespaces, mentions."
      ],
      "metadata": {
        "id": "lGGkaFhE360S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to lowercase"
      ],
      "metadata": {
        "id": "orAHqnwtcBH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_lowercase = df_complete.withColumn('text', lower(col('text')))\n",
        "df_lowercase.show(5)"
      ],
      "metadata": {
        "id": "Guj-V0Sa-VZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Links"
      ],
      "metadata": {
        "id": "gvABbixLcEEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_links = df_lowercase.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
        "df_links.show(5)"
      ],
      "metadata": {
        "id": "uVXPLbDXNq2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove mentions"
      ],
      "metadata": {
        "id": "kp2tHehscHav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mentions = df_links.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
        "df_mentions.show(5)"
      ],
      "metadata": {
        "id": "kNotR0WCNvZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove hashtag, keeping the word"
      ],
      "metadata": {
        "id": "ZdyQbi7tcKvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hashtag = df_mentions.withColumn('text', regexp_replace('text', '#', ''))\n",
        "df_hashtag.show(5)"
      ],
      "metadata": {
        "id": "U2m-waqGNzf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove RT symbol"
      ],
      "metadata": {
        "id": "tinvjmSacNMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_RT = df_hashtag.withColumn('text', regexp_replace('text', 'RT', ''))\n",
        "df_RT.show(5)"
      ],
      "metadata": {
        "id": "qsQzeAiqN2-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove punctuation"
      ],
      "metadata": {
        "id": "YTeyb52FcO-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_punctuation = df_RT.withColumn('text', regexp_replace('text', '[^a-zA-Z\\\\s]', ''))\n",
        "df_punctuation.show(5)"
      ],
      "metadata": {
        "id": "VR6VCqdaN5kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove new line symbol"
      ],
      "metadata": {
        "id": "bVxdMpKicR3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_line = df_punctuation.withColumn('text', regexp_replace('text', '\\n', ''))\n",
        "df_new_line.show(5)"
      ],
      "metadata": {
        "id": "CfBt3wKyOXRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove emoij"
      ],
      "metadata": {
        "id": "PFb46xXycTst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoij = df_new_line.withColumn('text', regexp_replace('text', \"[^\\x00-\\x7F]+\" , ''))\n",
        "df_emoij.show(5)"
      ],
      "metadata": {
        "id": "PSNeYYDlOa7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Digits"
      ],
      "metadata": {
        "id": "7VF5lLh3scuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_digit = df_emoij.withColumn('text', regexp_replace('text', r'[0-9]{5,}', ''))\n",
        "df_digit.show(5)"
      ],
      "metadata": {
        "id": "t6bZWfWkseEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spell Checker"
      ],
      "metadata": {
        "id": "NtdPyrJnmnlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When annotating the tweets, I've noticed that many of them contained spelling errors. It is recommended to adjust those tweets before the model training."
      ],
      "metadata": {
        "id": "J8fxGMrPZekd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "broker = enchant.Broker()\n",
        "broker.describe()\n",
        "broker.list_languages()"
      ],
      "metadata": {
        "id": "mmwTu7-xsNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_checker(text):\n",
        "  checker = SpellChecker(\"it_IT\", text)\n",
        "  for err in checker:\n",
        "    if len(err.suggest())>0:\n",
        "      sug = err.suggest()[0]\n",
        "      err.replace(sug)\n",
        "  return checker.get_text()"
      ],
      "metadata": {
        "id": "dDwr9bgG5e4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "udf_spell_checker = udf(lambda x: spell_checker(x), StringType())\n",
        "df_spell = df_digit.withColumn('text', udf_spell_checker(col('text')))\n",
        "\n",
        "df_spell.cache()\n",
        "\n",
        "df_spell.show(5)"
      ],
      "metadata": {
        "id": "zxp3RisBN48j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing exceeding whitespace"
      ],
      "metadata": {
        "id": "mMABXUG0cWl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a. Trimming\")\n",
        "df_trimming = df_spell.withColumn('text', trim(col('text')))\n",
        "df_trimming.show(5, truncate=False)\n",
        "\n",
        "print(\"b. Filter out extra whitespaces\")\n",
        "df_cleaned = df_trimming.withColumn('text', regexp_replace(col(\"text\"), \" +\", \" \"))\n",
        "\n",
        "df_cleaned.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "BR_rP0QqOeYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "aJX-UZe_aBHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_cleaned.select([col('text'), col('sarcastic')])\n",
        "\n",
        "df.cache()\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "df_spell.unpersist()"
      ],
      "metadata": {
        "id": "W2E3VSNhjtE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Feature Engineering**"
      ],
      "metadata": {
        "id": "p-cnH0NX7Xup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting feature engineering, constructing pipeline..\")"
      ],
      "metadata": {
        "id": "OjDEMuzht4po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document assembler\n",
        "Each annotator in Spark NLP takes specific sorts of columns and produces new columns of a different type. We have the following types in Spark NLP: document, token, chunk, pos, word embeddings, date, entity, sentiment, named entity, dependency, labeled dependency.\n",
        "\n",
        "To implement the solution in Spark NLP, we must first transform raw data into Document type. DocumentAssembler() is a special transformer that builds the initial annotation of type Document that annotators can utilize later on."
      ],
      "metadata": {
        "id": "rAS_atsEaqLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "                        .setInputCol('text')\\\n",
        "                        .setOutputCol('document')\\\n",
        "                        .setCleanupMode(\"shrink\")"
      ],
      "metadata": {
        "id": "WGJ6c4YZjhi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "Tokenization is the process of breaking raw text into smaller pieces. Tokenization divides the raw text into words known as tokens. These tokens help to better understand the context or constructing the NLP model. Tokenization aids in determining the meaning of the text by evaluating the word sequence."
      ],
      "metadata": {
        "id": "tqvryDZkbHWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = sparknlp.annotator.Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "CG1Hu0uBPpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizer\n",
        "Lemmatization is a technique for reducing words to their normalized form. The transformation of lemmatization employs a dictionary to map distinct versions of a word back to its base format. So, using this method, we may reduce non-trivial inflections like \"is,\" \"was,\" and \"were\" down to the root \"be.\""
      ],
      "metadata": {
        "id": "cJ_NmjWhb11e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = Lemmatizer()\\\n",
        "     .setInputCols(['token'])\\\n",
        "     .setOutputCol('lemma')\\\n",
        "     .setDictionary(\"lemmatization-it.txt\", \"->\", \"\\t\")"
      ],
      "metadata": {
        "id": "FkKFHCWrP6pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords cleaner\n",
        "Removes stopwords, that are not useful to our goal, from the text."
      ],
      "metadata": {
        "id": "wltZjHU7cb9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_it\", \"it\")\\\n",
        "     .setInputCols(['lemma'])\\\n",
        "     .setOutputCol('clean_lemma')"
      ],
      "metadata": {
        "id": "LZhcIFXqP2ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding using BERT pretrained for italian language\n",
        "Word Embedding is a method that involves representing a word with a vector. The BERT model was used to construct these embeddings in the code below since it provides embeddings that allow us to have numerous vector representations for the same word dependent on the context in which the word is used. BERT embeddings are thus context-dependent."
      ],
      "metadata": {
        "id": "V5Ztn9KYcjdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = BertEmbeddings.pretrained(\"bert_base_italian_cased\", \"it\") \\\n",
        "      .setInputCols([\"document\", \"clean_lemma\"]) \\\n",
        "      .setOutputCol(\"embeddings\")"
      ],
      "metadata": {
        "id": "_SMA5vO8P9Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = SentenceEmbeddings()\\\n",
        "                        .setInputCols([\"document\", \"embeddings\"])\\\n",
        "                        .setOutputCol(\"sentence_embeddings\")"
      ],
      "metadata": {
        "id": "Vw4DouOoisXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingsFinisher = EmbeddingsFinisher() \\\n",
        "                      .setInputCols(\"sentence_embeddings\") \\\n",
        "                      .setOutputCols(\"finished_sentence_embeddings\") \\\n",
        "                      .setOutputAsVector(True) \\\n",
        "                      .setCleanAnnotations(False)"
      ],
      "metadata": {
        "id": "w7f-XHIFGZu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting pipeline"
      ],
      "metadata": {
        "id": "apVjUtRcgUi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[document_assembler,\n",
        "                            tokenizer,\n",
        "                            lemma,\n",
        "                            stopwords_cleaner,\n",
        "                            embeddings,\n",
        "                            sentence_embeddings,\n",
        "                            embeddingsFinisher\n",
        "                            ])"
      ],
      "metadata": {
        "id": "Z1lrAgu3QC8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "features = pipeline.fit(df)"
      ],
      "metadata": {
        "id": "i284tX2NRJGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = features.transform(df)"
      ],
      "metadata": {
        "id": "AF-axIuw7Zfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.cache()\n",
        "print(\"word embeddings\")\n",
        "embeddings.select('embeddings').show(5, truncate=False)\n",
        "print(\"sentence embeddings\")\n",
        "embeddings.select('sentence_embeddings').show(5, truncate=False)\n",
        "print(\"finisher\")\n",
        "embeddings.select('finished_sentence_embeddings').show(5, truncate=False)"
      ],
      "metadata": {
        "id": "9KXs6UYY8zLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()"
      ],
      "metadata": {
        "id": "QFIgtTKg9FTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "AKicmZse8UWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas = embeddings.to_pandas_on_spark()\n",
        "df_pandas.head(5)"
      ],
      "metadata": {
        "id": "Jjxoc1MQMLYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.unpersist()"
      ],
      "metadata": {
        "id": "-7A4GwmLyQHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = df_pandas['finished_sentence_embeddings']\n",
        "target = df_pandas['sarcastic']"
      ],
      "metadata": {
        "id": "E54zyT5Hv9IF"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features,\n",
        "                                                    target,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42,\n",
        "                                                    shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "enHy4ZwvwFak",
        "outputId": "9e3d4111-850f-4891-ba25-626167852834"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-93-af41aa40cbd9>\", line 4, in <module>\n",
            "    random_state=42)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\", line 2417, in train_test_split\n",
            "    arrays = indexable(*arrays)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 378, in indexable\n",
            "    check_consistent_length(*result)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 329, in check_consistent_length\n",
            "    lengths = [_num_samples(X) for X in arrays if X is not None]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 329, in <listcomp>\n",
            "    lengths = [_num_samples(X) for X in arrays if X is not None]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 267, in _num_samples\n",
            "    if hasattr(x, \"shape\") and x.shape is not None:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/pandas/series.py\", line 1043, in shape\n",
            "    return (len(self),)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/pandas/base.py\", line 432, in __len__\n",
            "    return len(self._psdf)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/pandas/frame.py\", line 11909, in __len__\n",
            "    return self._internal.resolved_copy.spark_frame.count()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\", line 680, in count\n",
            "    return int(self._jdf.count())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1308, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
            "    newpath = join(path, name)\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 89, in join\n",
            "    elif not path or path.endswith(sep):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(12))\n",
        "model.compile(optimizer='adam', loss=\"mse\", metrics=['mse', 'mae', 'mape'])\n",
        "\n",
        "# callback stops the traning when the val_loss is increasing\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# fit the model with a validation dataset\n",
        "base_history = model.fit(x_train[:, 1:].astype('float64'), y_train, epochs=30, batch_size=64, verbose=2,\n",
        "                          validation_split=0.2, callbacks=[callback])"
      ],
      "metadata": {
        "id": "qLHLGExG8XFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}