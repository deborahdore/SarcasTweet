{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARCASTWIT_word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DokjWUCvcIkQ",
        "5wh23uCQcxfP",
        "29_DCW9URUjb",
        "daSEOScGUKJo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Define dependencies and constrains**"
      ],
      "metadata": {
        "id": "IfcrKr5Na8A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to download tweet from Twitter, first one must create an account and apply for **developer priviledges**. The application will grant the developer basic access the the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) which are not enough because it only allows the download of tweet of the last 7 days. Therefore, I've applied to the [Premium plan](https://developer.twitter.com/en/support/twitter-api/premium) which allows the download of 25k of tweets per month along with the use _full archive_ and the _30 days_ search API but with limited amout of request per month."
      ],
      "metadata": {
        "id": "TdELnf08cHP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ImtN6rXUazit"
      },
      "outputs": [],
      "source": [
        "COLAB_DIR = \"/content/\"\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# File with Twitter project credentials\n",
        "CREDENTIALS = COLAB_DIR+'credentials.yaml'\n",
        "CREDENTIALS_KEY = 'search_tweets_30_day_dev'\n",
        "\n",
        "# csv file where tweet downloaded will be saved\n",
        "DATASET = COLAB_DIR+'dataset.csv'\n",
        "DATASET_ANNOTATED = COLAB_DIR+'dataset_annotated.csv'\n",
        "SENTIPOLIC = COLAB_DIR+'sentipolic.csv'\n",
        "\n",
        "# result of data processing\n",
        "DATASET_PROCESSED = COLAB_DIR+\"result_processed\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0sYKlPLLjvx",
        "outputId": "bebe8874-43c8-4035-8c6d-aadadf99e90e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install libraries"
      ],
      "metadata": {
        "id": "bdKPmsLJTYA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libenchant1c2a\n",
        "!pip install pyenchant\n",
        "!apt-get install hunspell-it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdt8lR_QBTm",
        "outputId": "93369d26-9c14-4596-adba-6e7de84ee932"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant1c2a is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "hunspell-it is already the newest version (1:6.0.3-3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7hhURdQB9v",
        "outputId": "39de6bd5-014b-4ad4-cdee-192cbf10328a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GbgJr6MgmM",
        "outputId": "bca778cc-3e0c-4065-e97b-b37778d98447"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['ARROW_PRE_0_15_IPC_FORMAT'] = '1'"
      ],
      "metadata": {
        "id": "dVTyQPmzDff1"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2\n",
        "!pip install spark-nlp==3.4.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvmO7zGx6v_1",
        "outputId": "236180eb-6339-45e8-9ab2-71c8c04fd32d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/configuration.py\", line 21, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
            "    from pip._vendor.pkg_resources import Distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3251, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3235, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3279, in _initialize_master_working_set\n",
            "    for dist in working_set\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3279, in <genexpr>\n",
            "    for dist in working_set\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2783, in activate\n",
            "    for pkg in self._get_metadata('namespace_packages.txt'):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2766, in _get_metadata\n",
            "    if self.has_metadata(name):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1414, in has_metadata\n",
            "    return self._has(path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1603, in _has\n",
            "    def _has(self, path):\n",
            "KeyboardInterrupt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spark-nlp==3.4.4 in /usr/local/lib/python3.7/dist-packages (3.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNZLejN9TRN2",
        "outputId": "7b7d1006-b877-413b-baa5-8e6396db7ba0"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tqdm in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (2.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (4.64.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparktorch"
      ],
      "metadata": {
        "id": "riiFxATzw3gl",
        "outputId": "06a51712-e9c7-4478-bd92-63379b69f624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sparktorch in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from sparktorch) (2.23.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from sparktorch) (1.1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sparktorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from sparktorch) (0.3.5.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->sparktorch) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->sparktorch) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->sparktorch) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->sparktorch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark configurations"
      ],
      "metadata": {
        "id": "VdfBlXxLTgxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark packages\n",
        "from pyspark import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.feature import VectorAssembler, SQLTransformer, Normalizer\n",
        "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, transform\n",
        "\n",
        "import sparknlp"
      ],
      "metadata": {
        "id": "V-dnnL8JS90g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "OJzJeXfRMCPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark NLP\")\\\n",
        "    .master(\"local[4]\")\\\n",
        "    .config('spark.executor.memory', '8G')\\\n",
        "    .config(\"spark.driver.memory\",\"100G\")\\\n",
        "    .config(\"spark.driver.cores\", \"5\")\\\n",
        "    .config(\"spark.scheduler.mode\", \"FAIR\")\\\n",
        "    .config(\"spark.driver.maxResultSize\", \"10G\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)"
      ],
      "metadata": {
        "id": "mkCNpht6bvLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "type(sc)"
      ],
      "metadata": {
        "id": "HYCtzalg-rGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd ~/.ivy2/cache/com.johnsnowlabs.nlp/spark-nlp_2.12/jars && ls -lt"
      ],
      "metadata": {
        "id": "0yAIKiS36-3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Files from GitHub"
      ],
      "metadata": {
        "id": "DokjWUCvcIkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/dataset.csv\n",
        "!wget https://github.com/deborahdore/italian-sarcastic-tweet-classification/raw/main/dataset/other/sentipolic.csv\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/credentials/credentials.yaml\n",
        "!wget https://raw.githubusercontent.com/deborahdore/italian-sarcastic-tweet-classification/main/dataset/dataset_annotated.csv"
      ],
      "metadata": {
        "id": "EENgh8yncNzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# italian dictionary for lemmatization\n",
        "!wget https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-it.txt"
      ],
      "metadata": {
        "id": "5v7zGUIlnl4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Retrieve Tweet**\n",
        "\n",
        "\n",
        "> Following, some code cell will be annotated with *%% script false* in order to avoid their execution. Those cell concern the download of the tweets from Twitter. Even if this may not sound dangerous, I've finished the request at my disposal. Therefore, calling the Twitter API will produce an error. Also, please don't run them otherwise the output of the cell will be lost.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wh23uCQcxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# useful imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import requests\n",
        "import json\n",
        "import yaml\n",
        "import csv\n",
        "import pdb\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cynI3DutEnEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First we must retrieve and validate the credentials that we will need to access the Twitter API. I've store the bearer token in a yaml file: *credentials.yaml*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gt6JZtFie0Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_credentials(credentials, key):\n",
        "  with open(credentials, \"r\") as stream:\n",
        "    try:\n",
        "        credentials = yaml.safe_load(stream)\n",
        "        return credentials[key]\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)"
      ],
      "metadata": {
        "id": "qOoIg7uZc2bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = handle_credentials(CREDENTIALS, CREDENTIALS_KEY)\n",
        "endpoint = credentials['endpoint'] # we will use this endpoint to search for the tweet\n",
        "print(endpoint)"
      ],
      "metadata": {
        "id": "i_UEONFUfOt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Second we must create the header for the request"
      ],
      "metadata": {
        "id": "J-RByZFOf9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_headers(credentials:dict):\n",
        "  headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {credentials[\"bearer_token\"]}'\n",
        "  }\n",
        "  return headers"
      ],
      "metadata": {
        "id": "5OYEqkb2fW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = handle_headers(credentials)\n",
        "headers"
      ],
      "metadata": {
        "id": "vfRm7V29gCJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Another parameter of the request is the query. The query determines which tweet will be returned in the response. In our case, we have 2 types of queries: the one that searches for sarcastic tweets and the one that returns non-sarcastic tweets"
      ],
      "metadata": {
        "id": "9yp2tFf9gOsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the query about sarcastic tweet I've chosen some keyword that, in my opion, are used to express sarcasm and/or irony (sarcasm is a sub-type of irony):\n",
        "\n",
        "\n",
        "1. sarcasmo (with or without #)\n",
        "2. ironia (with or without #)\n",
        "3. \"*ridiamo per non piangere*\"\n",
        "4. #coincidenze (.. io non credo) is mostly used to express sarcasm\n",
        "5. \"*qualquadra non cosa*\"\n",
        "\n",
        "Many studies also suggest that sarcasm can be found in tweet related to politics. Therefore, these seems very good starting point:\n",
        "1. monti, draghi, berlusconi (known italian prime minister)\n",
        "2. governo\n",
        "3. premier\n",
        "\n",
        "\n",
        "For non-sarcastic tweet, I've excluded all the possibile word that may refer to sarcasm.\n",
        "\n",
        "The list of operator used can be found in the [Twitter API documentation](https://developer.twitter.com/en/docs/twitter-api/enterprise/rules-and-filtering/operators-by-product)."
      ],
      "metadata": {
        "id": "jL_QG_G5Ew4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_query = '(#sarcasmo OR sarcasmo OR #ironia OR ironia OR \"ridiamo per non piangere\" \\\n",
        "                  OR #coincidenze OR \"qualquadra non cosa\" OR draghi OR monti OR berlusconi \\\n",
        "                  OR governo OR premier) lang:it -has:media'\n",
        "\n",
        "non_sarcasm_query = '-\"ridiamo per non piangere\" -sarcasmo -ironia -\"qualquadra non cosa\" lang:it -has:media'"
      ],
      "metadata": {
        "id": "B4Qan_sigKsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we can define the function that will handle the request and the dataframe where tweet will be stored.\n",
        "\n",
        "\n",
        "> Other parameters that we need in order to process the request are:\n",
        "- *max_result_per_page* : the maximum number of tweets per call \n",
        "- *next_token* : a token that if passed to the request will return the next page of results\n",
        "- I've defined a parameter *max_num_of_request* that will stop the call once that we've reached the desidered amount of calls. This must be done because the request at our disposal are not illimited. So we must be careful to the number of the request that we do\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1AItsWZphAXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_request(endpoint, headers, query, max_result_per_page, next_token = None):\n",
        "  \n",
        "  if next_token is not None:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "      \"next\": next_token\n",
        "    })\n",
        "  else:\n",
        "    payload = json.dumps({\n",
        "      \"maxResults\": max_result_per_page,\n",
        "      \"query\": query,\n",
        "    })\n",
        "  \n",
        "  response = requests.post(endpoint, headers=headers, data=payload)\n",
        "\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "OVmgNDDch8Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tweet(response, label):\n",
        "  tweets = []\n",
        "  json_response = json.loads(response)\n",
        "  \n",
        "  if 'results' in response:\n",
        "    results = json_response[\"results\"]\n",
        "\n",
        "    for tweet in results:\n",
        "      # is tweet a retweet?\n",
        "      if 'retweeted_status' in tweet:\n",
        "        if tweet['retweeted_status']['truncated']:\n",
        "          text = tweet['retweeted_status']['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['retweeted_status']['text']\n",
        "      else:\n",
        "        if tweet['truncated']:\n",
        "          text = tweet['extended_tweet']['full_text']\n",
        "        else:\n",
        "          text = tweet['text']\n",
        "        \n",
        "      text = text.replace('\"', \"'\")\n",
        "      data = Tweet(tweet[\"id\"], f\"{text}\", label)\n",
        "      \n",
        "      tweets.append(data)\n",
        "\n",
        "  else:\n",
        "    print(\"Request went wrong\")\n",
        "    print(response)\n",
        "\n",
        "  return tweets"
      ],
      "metadata": {
        "id": "1gNdo7Bnijtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   query, \n",
        "                   label,\n",
        "                   max_result_per_page,\n",
        "                   tweet_list,\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 20):\n",
        "\n",
        "  if max_num_of_request <= 0:\n",
        "    return tweet_list\n",
        "\n",
        "  response = handle_request(endpoint, headers, query, max_result_per_page, next_token)\n",
        "\n",
        "  tweet_list.extend(extract_tweet(response, label))\n",
        "\n",
        "  try:\n",
        "      next_token = json.loads(response)['next']\n",
        "  except:\n",
        "      next_token = None\n",
        "\n",
        "  if next_token is not None:\n",
        "      return download_tweet(endpoint, headers, query, label, max_result_per_page,\n",
        "                   tweet_list, next_token, max_num_of_request - 1)\n",
        "  else:\n",
        "      return tweet_list"
      ],
      "metadata": {
        "id": "oHbiTTbNg7x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tweet\n",
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "yUW4eUSoyq08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []"
      ],
      "metadata": {
        "id": "nnd9wSFdmzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download sarcastic tweet\n",
        "tweets = download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   sarcasm_query, \n",
        "                   \"Yes\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40)"
      ],
      "metadata": {
        "id": "wqphEQDcwHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# download non-sarcastic tweet\n",
        "tweets.extend(\n",
        "    download_tweet(endpoint, \n",
        "                   headers, \n",
        "                   non_sarcasm_query, \n",
        "                   \"No\",\n",
        "                   100,\n",
        "                   [],\n",
        "                   next_token = None, \n",
        "                   max_num_of_request = 40))"
      ],
      "metadata": {
        "id": "rLUvtri88LyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(tweets)"
      ],
      "metadata": {
        "id": "RZeSJmGt5FOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "nmS3eNL075po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "\n",
        "# create file\n",
        "if not os.path.exists(DATASET):\n",
        "  os.mknod(DATASET)\n",
        "\n",
        "# save tweets\n",
        "df.toPandas().to_csv(DATASET, header=True, index=False) "
      ],
      "metadata": {
        "id": "PgAKUhbDmrmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Annotate Tweet**"
      ],
      "metadata": {
        "id": "29_DCW9URUjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# python widgets\n",
        "from ipywidgets import Button\n",
        "import asyncio\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import HBox, Layout\n",
        "import time as t"
      ],
      "metadata": {
        "id": "SXY6DWppEFJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we download tweet using an hashtag, we are not 100% sure of what we downloaded is correct. We must analyze - at least - the majority of the tweet to understand if what we have labelled is correct. There here's a little tool to help us with that."
      ],
      "metadata": {
        "id": "xrDr46suRbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet = Row(\"id\", \"text\", \"sarcastic\")\n",
        "\n",
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df = spark.createDataFrame(pd.read_csv(DATASET), schema=schema)\n",
        "df.show(10)"
      ],
      "metadata": {
        "id": "8hS8XFLNRa2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_label(df, numeric=False):\n",
        "  label_yes = 1 if numeric else \"Yes\"\n",
        "  label_no = 0 if numeric else \"No\"\n",
        "  return df.groupBy(\"sarcastic\").agg(\n",
        "      count(when(col(\"sarcastic\") == label_yes, 1)),\n",
        "      count(when(col(\"sarcastic\") == label_no, 1)))"
      ],
      "metadata": {
        "id": "g-2CxhPEb59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count tweet\n",
        "print(f'Total number of tweet retrieved {df.count()}')"
      ],
      "metadata": {
        "id": "UWC7f6AHbzKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want first to drop duplicates\n",
        "\n",
        "print(\"Count before drop:\")\n",
        "count_label(df).show()\n",
        "\n",
        "count_before_drop = df.count()\n",
        "df = df.dropDuplicates([\"text\"])\n",
        "print(f\"Distinct count: {str(df.count())} \\n\")\n",
        "\n",
        "print(\"Count after drop:\")\n",
        "count_label(df).show()"
      ],
      "metadata": {
        "id": "gciiYKT7R148"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dropped {count_before_drop-df.count()} columns')\n",
        "print(f'total count: {df.count()}')"
      ],
      "metadata": {
        "id": "q3WedH78qpMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visually \n",
        "data = count_label(df).collect()\n",
        "\n",
        "labels = ['sarcastic', 'non sarcastic']\n",
        "colors = sns.color_palette('pastel')[0:5]\n",
        "\n",
        "plt.pie([int(data[1][1]), int(data[0][2])], labels = labels, colors = colors, autopct='%.0f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "idZSd7XIVbXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_annotated = []"
      ],
      "metadata": {
        "id": "t4hgqk7qs9mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_change(widget1, widget2): \n",
        "    future = asyncio.Future()\n",
        "    def getvalue(change):\n",
        "        future.set_result(change.description)\n",
        "        widget1.on_click(getvalue, remove=True)\n",
        "        widget2.on_click(getvalue, remove=True) \n",
        "    widget1.on_click(getvalue)\n",
        "    widget2.on_click(getvalue)\n",
        "    return future\n",
        "\n",
        "async def f(df):\n",
        "  df_pandas = df.toPandas()\n",
        "  for index, row in df_pandas.iterrows():\n",
        "    print(f'Is this tweet sarcastic? \\n {row.text} \\n', flush=True)\n",
        "\n",
        "    x = await wait_for_change(sarcastic,non_sarcastic)\n",
        "    \n",
        "    if x == \"Yes\":\n",
        "      print(\"Tagged \", row.id, \"with sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"Yes\")\n",
        "      tweets_annotated.append(data)\n",
        "    else:\n",
        "      print(\"Tagged \", row.id, \"with non-sarcastic \\n\")\n",
        "      data = Tweet(row.id, row.text, \"No\")      \n",
        "      tweets_annotated.append(data)\n",
        "\n",
        "    clear_output()\n",
        "    display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "T3QQC7zoSvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going forward, we want to ask ourselves *How can know if a tweet is sarcastic or not?*\n",
        "\n",
        "*In Harry Potter and the Half Blood Prince, there is a scene where Harry is leaving the Weasley house and Mrs. Weasley says: “Promise me you will look after yourself…stay out of trouble….” Harry responds: “I always do Mrs. Weasley. I like a quiet life, you know me.” Anyone familiar with Harry Potter knows that his life is far from quiet, and so he must not really mean what he is saying. In fact, Harry is being sarcastic.*\n",
        "\n",
        "[source](https://kids.frontiersin.org/articles/10.3389/frym.2018.00056)\n",
        "\n",
        "Sarcasm is the use of words that say the opposite of what you really mean, often as a joke and with a tone of voice that shows this. It is often used to mock or critize someone, express disapproval or as a defence mechanism.\n",
        "\n",
        "For example:\n",
        "> *Noi invece ce la caviamo con un grado in meno ai termosifoni d'inverno e spegnendo i condizionatori d'estate. Non è fantastico? (#Draghi è un cialtrone sesquipedale, nel caso aveste ancora qualche dubbio)*\n",
        "\n",
        "Here we can imagine the sarcastic tone of the writer. He's obviously criticising the Italian prime minister, Mario Draghi, when, during an interview, he said that we must make sacrifices like lowering the grade of the radiator in order to cope with the possibility of not having the gas from Russia anymore. Obviously, this won't be enough. *Isn't this great?*\n",
        "\n",
        "Sometimes it's difficult also for a human person to understand sarcasm therefore I don't expect the following dataset to be 100% free from bias."
      ],
      "metadata": {
        "id": "8Kyh9cjvkoZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool used for annotation: it displays each tweet and the user has to click \"Yes\" \n",
        "# if the tweet was sarcastic, \"No\" otherwise\n",
        "\n",
        "sarcastic=Button(description=\"Yes\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "non_sarcastic=Button(description=\"No\", button_style='info', layout=Layout(width='150px', height='50px'))\n",
        "\n",
        "asyncio.create_task(f(df))\n",
        "t.sleep(2)\n",
        "display(HBox([sarcastic,non_sarcastic]))"
      ],
      "metadata": {
        "id": "NJM-MObHrwWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "print(tweets_annotated)"
      ],
      "metadata": {
        "id": "XvlYa09uXPRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "df_annotated = spark.createDataFrame(tweets_annotated)\n",
        "df_annotated.tail(5)"
      ],
      "metadata": {
        "id": "5oOAAGZFsYKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false\n",
        "if not os.path.exists(DATASET_ANNOTATED):\n",
        "  os.mknod(DATASET_ANNOTATED)\n",
        "\n",
        "# save tweets\n",
        "df_annotated.toPandas().to_csv(DATASET_ANNOTATED, header=True, index=False) "
      ],
      "metadata": {
        "id": "ClMe-OHFXrDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Extend Dataset**"
      ],
      "metadata": {
        "id": "daSEOScGUKJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([StructField(\"id\", StringType(), True)\\\n",
        "                   ,StructField(\"text\", StringType(), True)\\\n",
        "                   ,StructField(\"sarcastic\", StringType(), True)])\n",
        "\n",
        "df_annotated = spark.createDataFrame(pd.read_csv(DATASET_ANNOTATED), schema=schema)"
      ],
      "metadata": {
        "id": "W4g8iOQTxneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Annotated tweets: {df_annotated.count()}\")"
      ],
      "metadata": {
        "id": "K421QQz1sfyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the code below, we lost multiple *tweet*.\n",
        "First of all, multiple tweets classified as sarcastic were not sarcastic. Also, I've dropped every tweet that contained only one word, that wasn't actually in italian or \n",
        "that had no sense."
      ],
      "metadata": {
        "id": "M_awlh0kq3nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_annotated).show()"
      ],
      "metadata": {
        "id": "k6k_llNRpKCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can integrate we some external Dataset such as: [SENTIPOLIC](http://www.di.unito.it/~tutreeb/sentipolc-evalita16/index.html) from the challenge EVALITA2016 which contains several italian tweet already classified."
      ],
      "metadata": {
        "id": "UeH_4XijrNc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic = spark.createDataFrame(pd.read_csv(SENTIPOLIC))"
      ],
      "metadata": {
        "id": "B2eVp6uIw9kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "qld5kvU0yDBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will extract only the tweets which are ironic since we have plenty non-ironic\n",
        "df_sentipolic = df_sentipolic.filter(col(\"iro\")==1)"
      ],
      "metadata": {
        "id": "0LMvvrxNyFlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ironic tweet retrieved: {df_sentipolic.count()}\")"
      ],
      "metadata": {
        "id": "lN-MoSRiyQ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns that we don't need\n",
        "df_sentipolic = df_sentipolic.drop(*('subj', 'opos', 'oneg', 'lpos', 'lneg', 'top'))\n",
        "\n",
        "# rename columns\n",
        "df_sentipolic = df_sentipolic.withColumnRenamed(\"idTwitter\", \"id\")\\\n",
        "                              .withColumnRenamed(\"iro\", \"sarcastic\")\n",
        "\n",
        "# change order\n",
        "df_sentipolic = df_sentipolic.select(\"id\", \"text\", \"sarcastic\")"
      ],
      "metadata": {
        "id": "CmzhGiatyZxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentipolic.show(10)"
      ],
      "metadata": {
        "id": "PrDm7Oz_zbDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to join the two dataset. However we must use the same label for both.\n",
        "# Therefore if the tweet is sarcastic, the label will be 1, 0 otherwise.\n",
        "\n",
        "\n",
        "df_annotated = df_annotated.withColumn(\"sarcastic\", \n",
        "                                         when(df_annotated.sarcastic == \"Yes\", 1)\n",
        "                                         .when(df_annotated.sarcastic == \"No\", 0)                                    \n",
        "                                         .otherwise(df_annotated.sarcastic))"
      ],
      "metadata": {
        "id": "9J2Ekp4317OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotated.show()"
      ],
      "metadata": {
        "id": "MEk2bAHu2XwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate DataFrames\n",
        "\n",
        "df_complete = df_annotated.union(df_sentipolic)\n",
        "df_complete.show(5)"
      ],
      "metadata": {
        "id": "vRXyeqRH2nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Now we have a total of {df_complete.count()} tweets')"
      ],
      "metadata": {
        "id": "DGDmh6_d3Csy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_label(df_complete, numeric=True).show()"
      ],
      "metadata": {
        "id": "e89JUhcA3IkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is still unbalanced, but better than before."
      ],
      "metadata": {
        "id": "Ap6Pm9b13WEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Data Processing**"
      ],
      "metadata": {
        "id": "pDQJOzR4-Ncl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we want to clean tweet: remove hashtag, links, emoji, whitespaces, mentions."
      ],
      "metadata": {
        "id": "lGGkaFhE360S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to lowercase"
      ],
      "metadata": {
        "id": "orAHqnwtcBH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_lowercase = df_complete.withColumn('text', lower(col('text')))\n",
        "df_lowercase.show(5)"
      ],
      "metadata": {
        "id": "Guj-V0Sa-VZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Links"
      ],
      "metadata": {
        "id": "gvABbixLcEEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_links = df_lowercase.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
        "df_links.show(5)"
      ],
      "metadata": {
        "id": "uVXPLbDXNq2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove mentions"
      ],
      "metadata": {
        "id": "kp2tHehscHav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mentions = df_links.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
        "df_mentions.show(5)"
      ],
      "metadata": {
        "id": "kNotR0WCNvZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove hashtag, keeping the word"
      ],
      "metadata": {
        "id": "ZdyQbi7tcKvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hashtag = df_mentions.withColumn('text', regexp_replace('text', '#', ''))\n",
        "df_hashtag.show(5)"
      ],
      "metadata": {
        "id": "U2m-waqGNzf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove RT symbol"
      ],
      "metadata": {
        "id": "tinvjmSacNMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_RT = df_hashtag.withColumn('text', regexp_replace('text', 'RT', ''))\n",
        "df_RT.show(5)"
      ],
      "metadata": {
        "id": "qsQzeAiqN2-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove punctuation"
      ],
      "metadata": {
        "id": "YTeyb52FcO-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_punctuation = df_RT.withColumn('text', regexp_replace('text', '[^a-zA-Z\\\\s]', ''))\n",
        "df_punctuation.show(5)"
      ],
      "metadata": {
        "id": "VR6VCqdaN5kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove new line symbol"
      ],
      "metadata": {
        "id": "bVxdMpKicR3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_line = df_punctuation.withColumn('text', regexp_replace('text', '\\n', ''))\n",
        "df_new_line.show(5)"
      ],
      "metadata": {
        "id": "CfBt3wKyOXRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove emoij"
      ],
      "metadata": {
        "id": "PFb46xXycTst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoij = df_new_line.withColumn('text', regexp_replace('text', \"[^\\x00-\\x7F]+\" , ''))\n",
        "df_emoij.show(5)"
      ],
      "metadata": {
        "id": "PSNeYYDlOa7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Digits"
      ],
      "metadata": {
        "id": "7VF5lLh3scuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_digit = df_emoij.withColumn('text', regexp_replace('text', r'[0-9]{5,}', ''))\n",
        "df_digit.show(5)"
      ],
      "metadata": {
        "id": "t6bZWfWkseEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spell Checker"
      ],
      "metadata": {
        "id": "NtdPyrJnmnlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enchant\n",
        "from enchant.checker import SpellChecker"
      ],
      "metadata": {
        "id": "Gh3v2lF9D2LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When annotating the tweets, I've noticed that many of them contained spelling errors. It is recommended to adjust those tweets before the model training."
      ],
      "metadata": {
        "id": "J8fxGMrPZekd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "broker = enchant.Broker()\n",
        "broker.describe()\n",
        "broker.list_languages()"
      ],
      "metadata": {
        "id": "mmwTu7-xsNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_checker(text):\n",
        "  checker = SpellChecker(\"it_IT\", text)\n",
        "  for err in checker:\n",
        "    if len(err.suggest())>0:\n",
        "      sug = err.suggest()[0]\n",
        "      err.replace(sug)\n",
        "  return checker.get_text()"
      ],
      "metadata": {
        "id": "dDwr9bgG5e4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "udf_spell_checker = udf(lambda x: spell_checker(x), StringType())\n",
        "df_spell = df_digit.withColumn('text', udf_spell_checker(col('text')))\n",
        "\n",
        "df_spell.cache()\n",
        "\n",
        "df_spell.show(5)"
      ],
      "metadata": {
        "id": "zxp3RisBN48j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing exceeding whitespace"
      ],
      "metadata": {
        "id": "mMABXUG0cWl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a. Trimming\")\n",
        "df_trimming = df_spell.withColumn('text', trim(col('text')))\n",
        "df_trimming.show(5, truncate=False)\n",
        "\n",
        "print(\"b. Filter out extra whitespaces\")\n",
        "df_cleaned = df_trimming.withColumn('text', regexp_replace(col(\"text\"), \" +\", \" \"))\n",
        "\n",
        "df_cleaned.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "BR_rP0QqOeYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "aJX-UZe_aBHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_cleaned.select([col('text'), col('sarcastic')])\n",
        "\n",
        "df.cache()\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "df_spell.unpersist()"
      ],
      "metadata": {
        "id": "W2E3VSNhjtE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Feature Engineering**"
      ],
      "metadata": {
        "id": "p-cnH0NX7Xup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries for feature engineering\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from pyspark.ml.functions import vector_to_array"
      ],
      "metadata": {
        "id": "WNmH5S8BD-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting feature engineering, constructing pipeline..\")"
      ],
      "metadata": {
        "id": "OjDEMuzht4po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document assembler\n",
        "Each annotator in Spark NLP takes specific sorts of columns and produces new columns of a different type. We have the following types in Spark NLP: document, token, chunk, pos, word embeddings, date, entity, sentiment, named entity, dependency, labeled dependency.\n",
        "\n",
        "To implement the solution in Spark NLP, we must first transform raw data into Document type. DocumentAssembler() is a special transformer that builds the initial annotation of type Document that annotators can utilize later on."
      ],
      "metadata": {
        "id": "rAS_atsEaqLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "                        .setInputCol('text')\\\n",
        "                        .setOutputCol('document')\\\n",
        "                        .setCleanupMode(\"shrink\")"
      ],
      "metadata": {
        "id": "WGJ6c4YZjhi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Detector\n",
        "Finds sentence bounds in raw text."
      ],
      "metadata": {
        "id": "z-SPkWFVn953"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_detector = SentenceDetector()\\\n",
        "                      .setInputCols('document')\\\n",
        "                      .setOutputCol('sentence')"
      ],
      "metadata": {
        "id": "x1akLd6gbWHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "Tokenization is the process of breaking raw text into smaller pieces. Tokenization divides the raw text into words known as tokens. These tokens help to better understand the context or constructing the NLP model. Tokenization aids in determining the meaning of the text by evaluating the word sequence."
      ],
      "metadata": {
        "id": "tqvryDZkbHWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = sparknlp.annotator.Tokenizer()\\\n",
        "                    .setInputCols([\"sentence\"])\\\n",
        "                    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "CG1Hu0uBPpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizer\n",
        "Lemmatization is a technique for reducing words to their normalized form. The transformation of lemmatization employs a dictionary to map distinct versions of a word back to its base format. So, using this method, we may reduce non-trivial inflections like \"is,\" \"was,\" and \"were\" down to the root \"be.\""
      ],
      "metadata": {
        "id": "cJ_NmjWhb11e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer()\\\n",
        "     .setInputCols(['token'])\\\n",
        "     .setOutputCol('lemma')\\\n",
        "     .setDictionary(\"lemmatization-it.txt\", \"->\", \"\\t\")"
      ],
      "metadata": {
        "id": "FkKFHCWrP6pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords cleaner\n",
        "Removes stopwords, that are not useful to our goal, from the text."
      ],
      "metadata": {
        "id": "wltZjHU7cb9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_it\", \"it\")\\\n",
        "     .setInputCols(['lemma'])\\\n",
        "     .setOutputCol('clean_lemma')"
      ],
      "metadata": {
        "id": "LZhcIFXqP2ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finisher"
      ],
      "metadata": {
        "id": "qoyHv0giEuy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher()\\\n",
        "                  .setInputCols(\"clean_lemma\")\\\n",
        "                  .setOutputCols(\"pipeline_result\")"
      ],
      "metadata": {
        "id": "1vQJLKJyEw5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting pipeline"
      ],
      "metadata": {
        "id": "apVjUtRcgUi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlpPipeline = Pipeline(stages=[document_assembler,\n",
        "                               sentence_detector,\n",
        "                               tokenizer,\n",
        "                               lemmatizer,\n",
        "                               stopwords_cleaner,\n",
        "                               finisher\n",
        "                               ])"
      ],
      "metadata": {
        "id": "nc557jkAZc0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df_fitted = nlpPipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "yFO3xWIiE42O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fitted.cache()\n",
        "df_fitted.show()"
      ],
      "metadata": {
        "id": "xbO31EzPE-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()"
      ],
      "metadata": {
        "id": "CIzMPF2sFAFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding \n",
        "Word Embedding is a method that involves representing a word with a vector."
      ],
      "metadata": {
        "id": "V5Ztn9KYcjdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec"
      ],
      "metadata": {
        "id": "AtKwCcmHFDmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = Word2Vec() \\\n",
        "            .setInputCol(\"pipeline_result\") \\\n",
        "            .setOutputCol(\"embeddings\")"
      ],
      "metadata": {
        "id": "rxRE7ZUQNTXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = embeddings.fit(df_fitted)"
      ],
      "metadata": {
        "id": "i284tX2NRJGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = word2vec_model.getVectors().count()\n",
        "print(f'Vocabulary size is {vocabulary_size}')"
      ],
      "metadata": {
        "id": "k4RtvVDtbHag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target = word2vec_model.transform(df_fitted)"
      ],
      "metadata": {
        "id": "9KXs6UYY8zLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = len(df_target.take(1)[0][1])\n",
        "print(df_target.take(1)[0]['embeddings'])\n",
        "print(f'Size of each vector embedded is {vector_size}')"
      ],
      "metadata": {
        "id": "aNJhEjwQIagN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target.cache()\n",
        "df_target.show(5)\n",
        "df_target = df_target.select([col('sarcastic').alias('label'), col('embeddings').alias('features')])\n",
        "\n",
        "\n",
        "df_fitted.unpersist()"
      ],
      "metadata": {
        "id": "JLtMfGPK20Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target = df_target.filter(~col('label').contains('NaN'))"
      ],
      "metadata": {
        "id": "H0zgErP5DNtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()\n",
        "\n",
        "df_target = df_target.where(num_nonzeros(\"features\") > 0)"
      ],
      "metadata": {
        "id": "haMm-i5swULX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target = df_target.withColumn(\"label\", col(\"label\").cast('Float'))"
      ],
      "metadata": {
        "id": "A1fWgTryHbBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target = df_target.withColumn(\"f\", vector_to_array(\"features\", \"float64\"))\\\n",
        "            .select([\"label\"] + [col(\"f\")[i] for i in range(vector_size)])"
      ],
      "metadata": {
        "id": "IDvEdFXTHhvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_target.write.option(\"header\", True).csv(DATASET_PROCESSED)"
      ],
      "metadata": {
        "id": "EgM0wuO4uWrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "AKicmZse8UWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras import optimizers, regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "0zjsuaGgDxM9"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparktorch import SparkTorch, serialize_torch_obj\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "09VRxOVRFb6k"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(DATASET_PROCESSED)"
      ],
      "metadata": {
        "id": "W3WGgxJXu4QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change col types for training\n",
        "columns_name = df_processed.columns\n",
        "\n",
        "for col_name in df_processed.columns:\n",
        "  df_processed = df_processed.withColumn((col_name), col(col_name).cast(LongType()))"
      ],
      "metadata": {
        "id": "UKSbLi6HvHNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df_processed.randomSplit([0.7, 0.3], 1234)"
      ],
      "metadata": {
        "id": "XXnGERVNRuYg"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing pipeline\n",
        "\n",
        "vector_assembler = VectorAssembler(inputCols=columns_name[1:], outputCol='features')"
      ],
      "metadata": {
        "id": "pQuhuemMvnjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple NN"
      ],
      "metadata": {
        "id": "emLWyVXvvbbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)\n",
        "\n",
        "num_class = 2\n",
        "emsize = 100\n",
        "model = TextClassificationModel(vocabulary_size, emsize, num_class).to(device)"
      ],
      "metadata": {
        "id": "Z5CXxnGEvdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_obj = serialize_torch_obj(\n",
        "    model=network,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam,\n",
        "    lr=0.001\n",
        ")"
      ],
      "metadata": {
        "id": "BzIJovl7v3C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_model = SparkTorch(\n",
        "    inputCol='features',\n",
        "    labelCol='label',\n",
        "    predictionCol='predictions',\n",
        "    torchObj=torch_obj,\n",
        "    iters=25,\n",
        "    miniBatch=32,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "kDaWtDkjv70Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Pipeline(stages=[vector_assembler, spark_model]).fit(train)"
      ],
      "metadata": {
        "id": "lP-VSDnMv-p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = p.transform(test).persist()\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"predictions\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Area Under the ROC curve = %g\" % evaluator.evaluate(predictions))"
      ],
      "metadata": {
        "id": "yKQsndGwwIcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "kydPGKd_FRNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        vocab_size = 2046\n",
        "        embedding_dim = 100\n",
        "        num_hidden_nodes = 32\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        hidden_dim = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.2\n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        outputs=self.act(dense_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Y3NrqYJjHFSH"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_obj = serialize_torch_obj(\n",
        "    model=LSTMClassifier,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam,\n",
        "    lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "Ye08tV-LQzr2"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_model = SparkTorch(\n",
        "    inputCol='features',\n",
        "    labelCol='label',\n",
        "    predictionCol='predictions',\n",
        "    torchObj=torch_obj,\n",
        "    iters=2,\n",
        "    miniBatch=32,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "aG_JfZS6RM-9"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Pipeline(stages=[vector_assembler, spark_model]).fit(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "3HViEpscG36P",
        "outputId": "b8886907-4805-4ec0-ae12-f05f1e464e93"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-b801e755c354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector_assembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparktorch/torch_distributed.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparktorch/distributed.py\u001b[0m in \u001b[0;36mtrain_distributed\u001b[0;34m(rdd, torch_obj, iters, partition_shuffles, verbose, mini_batch, validation_pct, world_size, device, early_stop_patience)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# Run model with barrier execution mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             state_dict = mapPartitionsWithIndex(\n\u001b[0;32m--> 254\u001b[0;31m                 rdd, lambda i, x: handle_model(\n\u001b[0m\u001b[1;32m    255\u001b[0m                     \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = p.transform(test).persist()\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"predictions\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Area Under the ROC curve = %g\" % evaluator.evaluate(predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I8S0QR7tR_4",
        "outputId": "83558688-febf-4f07-ab5d-d5c1dc67f6ff"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy = 0.5\n"
          ]
        }
      ]
    }
  ]
}